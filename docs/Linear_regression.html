<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Blog | Regresión Lineal</title>
  <meta name="description" content="Explicación clara y profesional de la regresión lineal." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./css/regresionlineal.css" />
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1 class="site-title">Blog De Regresión Lineal</h1>
      <nav class="site-nav">
        <a class="btn btn-home" href="./index.html" aria-label="Volver al portafolio">← Volver al Home</a>
      </nav>
    </div>
  </header>

  <main class="container article-container" role="main">
    <article class="post">
      <header class="post-header">
        <p class="post-tag">Machine Learning</p>
        <h2 class="post-title">Regresión Lineal: Fundamentos y Aplicación</h2>
        <p class="post-meta">Publicado el <time datetime="2025-08-20">20 Agosto 2025</time> · Autor: Juan Acevedo</p>
      </header>

      <section class="post-section">
        <h3>¿Qué es la Regresión Lineal?</h3>
        <p>
          La regresión lineal es un modelo matemático–estadístico que se encarga de usar valores numéricos continuos ya conocidos para predecir valores nuevos.
            ¿Cómo funciona? Imaginemos que somos astrónomos y tenemos información de algunos planetas, como su rotación y su órbita. Un día queremos predecir la órbita de otro planeta y pensamos: si logramos trazar una línea que pase por los valores de los planetas que ya conocemos, tal vez esa misma línea nos ayude a predecir el valor del siguiente.

            Pero claro, en la realidad los planetas no van a estar perfectamente alineados. Si dibujamos una recta, habrá algunos puntos que no caigan exactamente sobre ella. Esa diferencia entre la línea y el valor real de un planeta es lo que llamamos error.

            Lo que nos interesa es medir en promedio cuánto nos equivocamos. Para eso, comparamos la diferencia entre la línea y cada planeta, sumamos esas diferencias y las dividimos entre el número de datos que tenemos. Sin embargo, hay un detalle: las diferencias pueden ser positivas o negativas y se cancelarían entre sí. Para evitarlo, usamos una técnica: elevar cada diferencia al cuadrado. Así todos los errores se vuelven positivos y, además, los más grandes quedan más penalizados.

            De esta forma obtenemos una medida del error promedio de nuestra recta. Y ahora el objetivo es minimizar ese error.

            ¿Y cómo se logra? Aquí entra la optimización con cálculo diferencial. Lo que hacemos es ver cómo se mueve la recta cuando cambiamos sus dos parámetros:

            La pendiente (qué tan inclinada está).

            El intercepto (dónde cruza el eje vertical).

            Si derivamos nuestra medida de error con respecto a cada uno de esos parámetros, podemos ver hacia dónde moverlos para que la línea se ajuste mejor. Cuando llegamos a un punto en el que las derivadas son cero, significa que encontramos la mejor posición posible para la recta.

            Así, logramos que nuestra línea se acerque lo más posible a los planetas (o a los datos), y una vez la tenemos, podemos usarla para predecir nuevos valores de forma bastante precisa.
        </p>
      </section>

      <section class="post-section">
        <h3>Modelo Matemático</h3>
        <p>
          En su forma simple, el modelo se expresa como: y = β₀ + β₁x + ε, donde β₀ es el intercepto, β₁ la pendiente
          y ε el término de error. El entrenamiento busca estimar β₀ y β₁ para minimizar el error cuadrático medio (MSE).
        </p>
      </section>

      <section class="post-section code-demo">
        <h3>Ejemplo en Python (Scikit-Learn)</h3>
        <pre><code class="code-block">from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Datos de ejemplo
X = [[1],[2],[3],[4],[5]]
y = [3, 5, 7, 9, 11]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = LinearRegression()
modelo.fit(X_train, y_train)

pred = modelo.predict(X_test)
print("Coeficiente:", modelo.coef_[0])
print("Intercepto:", modelo.intercept_)
print("MSE:", mean_squared_error(y_test, pred))</code></pre>
      </section>

      <section class="post-section">
        <h3>Buenas Prácticas</h3>
        <ul class="best-practices">
          <li>Normalizar o estandarizar si las escalas son muy diferentes.</li>
          <li>Revisar multicolinealidad (VIF) en modelos multivariados.</li>
          <li>Analizar residuos para validar supuestos.</li>
          <li>Usar métricas adicionales: R² ajustado, MAE, MSE.</li>
          <li>Revisar correlaciones.</li>
        </ul>
      </section>

      <section class="post-section">
        <h3>Limitaciones</h3>
        <p>
          Asume linealidad, homocedasticidad y ausencia de correlación en errores. Cuando estas condiciones no se cumplen,
          el rendimiento y la interpretabilidad se degradan.
        </p>
      </section>

      <footer class="post-footer">
        <a class="btn btn-primary" href="./index.html">← Regresar al Portafolio</a>
      </footer>
    </article>
  </main>

  <footer class="site-footer">
    <div class="container footer-inner">
      <p>&copy; 2025 Juan Acevedo · Todos los derechos reservados.</p>
    </div>
  </footer>
</body>
</html>